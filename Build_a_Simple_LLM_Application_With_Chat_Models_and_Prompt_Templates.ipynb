{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKRTaY8YEY248QqZldTCbw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramahasiba/NLP/blob/LangChain/Build_a_Simple_LLM_Application_With_Chat_Models_and_Prompt_Templates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Build a Simple LLM Application with Chat Models and Prompt Templates](https://python.langchain.com/docs/tutorials/llm_chain/)"
      ],
      "metadata": {
        "id": "Qot1Qyo2sprf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "SDSbuiUCtKf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv -q\n",
        "!pip install transformers -q\n",
        "!pip install langchain -q"
      ],
      "metadata": {
        "id": "XUh6G_IGWT6Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain[groq]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZXA0mP3aRfX",
        "outputId": "2c1da10c-f61e-42d9-b2fb-e00a293dece6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "xAzhBln1tGpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_u_r74HVT-L",
        "outputId": "a31e5789-b9af-48ec-98b4-5a09190f159d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the Langsmith api key:··········\n",
            "Enter langsmith project name: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "from dotenv import load_dotenv\n",
        "import getpass\n",
        "\n",
        "try:\n",
        "  load_dotenv('.env')\n",
        "except ImportError:\n",
        "  print('No .env file found')\n",
        "\n",
        "# Setup LangSmith to be able to inspect what exactly goes inside my chain or agent\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
        "  os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
        "      prompt = \"Enter the Langsmith api key:\"\n",
        "  )\n",
        "\n",
        "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
        "  os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
        "      prompt = \"Enter langsmith project name: \"\n",
        "  )\n",
        "  if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
        "    os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')\n",
        "os.environ[\"HF_TOKEN\"] = os.getenv('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLM"
      ],
      "metadata": {
        "id": "-mLvnt9HtwCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model # Chat model is unstance of the runnable interface\n",
        "model_name = \"llama3-70b-8192\"\n",
        "\n",
        "model=init_chat_model(model_name, model_provider=\"groq\")"
      ],
      "metadata": {
        "id": "BvJ8iPb-bEHv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the follwoing from English to French\"),\n",
        "    HumanMessage(\"Hi, how are you?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)"
      ],
      "metadata": {
        "id": "Feq8IUXiZ21t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the invoke method takes inputs and generate output of type message objects. it accepts inputs of type String and the openAI format."
      ],
      "metadata": {
        "id": "Sqzv0Xqffnfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "lIZr6BT0ca6Y",
        "outputId": "3e732915-e0fb-4cb5-a374-8747bc8a2215"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 149);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"Hello, what is your name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5huxyfdf3Yn",
        "outputId": "f287b61a-95d6-4e6e-f69b-f824e881d224"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Nice to meet you! I don\\'t have a personal name, but I\\'m an AI designed to assist and communicate with users in a helpful and friendly way. You can call me \"Assistant\" or \"AI\" if you like! What brings you here today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 17, 'total_tokens': 71, 'completion_time': 0.154285714, 'prompt_time': 0.000506862, 'queue_time': 0.054027948000000006, 'total_time': 0.154792576}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--8c766189-9846-4a3f-b233-1c8fba6821f7-0', usage_metadata={'input_tokens': 17, 'output_tokens': 54, 'total_tokens': 71})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke([\n",
        "    {\"role\": \"user\", \"content\": \"Hello\"}\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZCGpheCf3RZ",
        "outputId": "9ae72b36-1b28-46a2-b62d-81de26a9b1aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'completion_time': 0.074285714, 'prompt_time': 0.000129829, 'queue_time': 0.054252281, 'total_time': 0.074415543}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--cdae7aa3-2b66-47a4-8d06-abcf9494704f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke([HumanMessage(\"Hello\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKYJ_qQtf3Of",
        "outputId": "e712bf58-21a8-4dfd-8df4-b09e9065f0bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'completion_time': 0.074285714, 'prompt_time': 0.000243578, 'queue_time': 0.054642602, 'total_time': 0.074529292}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--eeaa8424-1536-45ee-83fc-779db874f52a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming\n",
        "streaming is a mode of invokation that Runnnable models provides. This allows us to stream individual tokens from a chat model."
      ],
      "metadata": {
        "id": "wJdS31s3wsUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in model.stream(messages):\n",
        "  print(token.content, end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKom4nEWgZSq",
        "outputId": "697c9b27-7e86-4304-9396-ef504795361c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bonjour ,  comment  vas -t u ?  "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates\n",
        "The application usually contains logic and to pass and combine user input according to the logic, we format a template with user input.\n",
        "\n",
        "**Prompt Templates** are a concept inLangchain that is designed to assist with transforming user input into the LLM application logic."
      ],
      "metadata": {
        "id": "T0o9k9-0ypyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "Ev-TI-nwjnmn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"language\": \"French\",\n",
        "    \"text\": \"hi\"\n",
        "})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrsAUpLOnvo2",
        "outputId": "e503fc8d-1110-46f6-9b1a-f8f2b065e696"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into French', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJUcrUWUnvj-",
        "outputId": "9fe2efac-d2aa-44f2-f50a-d62c3c40af0a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into French', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4myPWVwoyc_",
        "outputId": "c9ae1dfe-25d2-4523-b34f-134e638a5732"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salut !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQriTcpd1y4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}