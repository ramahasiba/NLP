{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt0iLQsbo2tOnrDMp/qY3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramahasiba/NLP/blob/LangChain/Build_an_Extraction_Chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Build an Extraction Chain](https://python.langchain.com/docs/tutorials/extraction/)\n",
        "\n",
        "In this notebook, I used tool-calling features of chat models to extract structured information from unstructured text."
      ],
      "metadata": {
        "id": "c4ze_8ONrG1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "VH7mgjjzCP0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tvd5IxDCqdhS"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q dotenv"
      ],
      "metadata": {
        "id": "Ie4hWHoGtDWf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain[groq]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9LGK8z2zp1O",
        "outputId": "e7139fa7-daa1-4f3e-91f5-7caa448a93e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "kwhMfGc1Cj-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "from dotenv import load_dotenv\n",
        "import getpass\n",
        "\n",
        "try:\n",
        "  load_dotenv('.env')\n",
        "except ImportError:\n",
        "  print('No .env file found')\n",
        "\n",
        "# Setup LangSmith to be able to inspect what exactly goes inside my chain or agent\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
        "  os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
        "      prompt = \"Enter the Langsmith api key:\"\n",
        "  )\n",
        "\n",
        "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
        "  os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
        "      prompt = \"Enter langsmith project name: \"\n",
        "  )\n",
        "  if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
        "    os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')\n",
        "os.environ[\"HF_TOKEN\"] = os.getenv('HF_TOKEN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TWfvCBos1uM",
        "outputId": "f886ca9b-3f51-4e2f-88c4-fcd35e1eb26f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter langsmith project name: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Schema\n",
        "describe what information we want to extract from the text.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GurUgMg_Cmuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Person(BaseModel):\n",
        "  \"\"\"Information about a person.\"\"\"\n",
        "  name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "  hair_color: Optional[str] = Field(\n",
        "      default=None, description=\"The colot of the person's hair if known\"\n",
        "  )\n",
        "  height_in_meters: Optional[str] = Field(\n",
        "      default=None, description=\"Height measured inmeters\"\n",
        "  )"
      ],
      "metadata": {
        "id": "exHF9xYgtBCS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Optional` allows the model to output None if it doesn't know the answer. and this is for getting the best performance were we do force the model to return resuts if there is no information to be extracted."
      ],
      "metadata": {
        "id": "CRXhMIFOudQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Extractor\n",
        "here we create information extractor using the schema we defined above. we deine a custom prompt to provide instructions and any additional context. we can add examples into the prompt template to improve the extraction quality.\n"
      ],
      "metadata": {
        "id": "0HGbmkj_DsWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"you are an expert extraction algorithm.\"\n",
        "            \"Only extract relevant information from the text.\"\n",
        "            \"If you do not know the value of an attribute asked to extract, \"\n",
        "            \"return null for the attribute's value.\"\n",
        "        ),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "cOua9v4rtOnY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model # Chat model is unstance of the runnable interface\n",
        "model_name = \"llama3-70b-8192\"\n",
        "\n",
        "llm=init_chat_model(model_name, model_provider=\"groq\")"
      ],
      "metadata": {
        "id": "0Uq2Dl57tOk3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Person)"
      ],
      "metadata": {
        "id": "cSqGEoVXtOee"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Alan Smith is 6 feet tall and has blond hair\"\n",
        "prompt = prompt_template.invoke({\"text\": text})\n",
        "structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCK8uZNatOcG",
        "outputId": "7d431af8-b7d4-4d68-f5b9-85dffa4273e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Entities\n",
        "In most cases, we extract list of entities and this can be done using `Pydantic` by nesting models inside one another.    \n",
        "\n",
        "The doc-string in the schema is sent to the LLM as the description of the schema Person, and it may help to improve extraction results. attributes description which is inside the Field is used by the LLM as well.\n",
        "\n",
        "Having good desccription can help impove the extraction results."
      ],
      "metadata": {
        "id": "5fkao9knJ1cG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Person(BaseModel):\n",
        "  \"\"\"Information about person\"\"\"\n",
        "  name: Optional[str] = Field(default=None, description=\"he name of the person\")\n",
        "  hair_color: Optional[str] = Field(\n",
        "      default=None, description=\"The color of the person's hair if known\"\n",
        "  )\n",
        "  height_in_meters: Optional[str] = Field(\n",
        "      dfault=None, description=\"height measured in meters\"\n",
        "  )\n",
        "\n",
        "class Data(BaseModel):\n",
        "  \"\"\"Extracted data about people\"\"\"\n",
        "  # Creates a model so we can extract multiple entities\n",
        "\n",
        "  people: List[Person]"
      ],
      "metadata": {
        "id": "mZ4NDbAV0WoT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Data)\n",
        "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
        "prompt = prompt_template.invoke({\"text\": text})\n",
        "structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGRA-6qu6cnV",
        "outputId": "b5bc8358-5f5f-4e5d-f547-033645587ea5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steered the model using few-shot prompting"
      ],
      "metadata": {
        "id": "azA6lY6GM3BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"2 🦜 2\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"4\"},\n",
        "    {\"role\": \"user\", \"content\": \"2 🦜 3\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"5\"},\n",
        "    {\"role\": \"user\", \"content\": \"3 🦜 4\"},\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skMGiocI6ckm",
        "outputId": "33b15075-b2b1-4d84-8d07-90c016090099"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converts a tool call example into a sequence of chat messages"
      ],
      "metadata": {
        "id": "snwyWHungG_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import tool_example_to_messages\n",
        "\n",
        "examples = [\n",
        "    (\n",
        "        \"The ocean is vast and blue. It's more than 20,000 feet deep.\",\n",
        "        Data(people=[])\n",
        "    ),\n",
        "    (\n",
        "        \"Fiona traveled far from France to Spain.\",\n",
        "        Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)])\n",
        "    )\n",
        "]\n",
        "\n",
        "messages = []\n",
        "\n",
        "for txt, tool_call in examples:\n",
        "  if tool_call.people:\n",
        "    # This final message is optional for some providers\n",
        "    ai_response = \"Detected people.\"\n",
        "  else:\n",
        "    ai_response = \"Dtected no people.\"\n",
        "  messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEXJPqIW6cZW",
        "outputId": "6f3d27bb-681c-4f83-e70a-9772f50e2956"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-3072253795.py:22: LangChainBetaWarning: The function `tool_example_to_messages` is in beta. It is actively being worked on, so the API may change.\n",
            "  messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in messages:\n",
        "  message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knEM3HXHOUu1",
        "outputId": "1b1b6b0d-ba36-42ec-9810-f3a10316b5f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "The ocean is vast and blue. It's more than 20,000 feet deep.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Data (27f27dab-63b6-40ce-afbd-45a3f4e934ff)\n",
            " Call ID: 27f27dab-63b6-40ce-afbd-45a3f4e934ff\n",
            "  Args:\n",
            "    people: []\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "You have correctly called this tool.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Dtected no people.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Fiona traveled far from France to Spain.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Data (a40db5de-90f5-46fd-970a-6864e9dafbe3)\n",
            " Call ID: a40db5de-90f5-46fd-970a-6864e9dafbe3\n",
            "  Args:\n",
            "    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "You have correctly called this tool.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Detected people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_no_extraction = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"The solar system is large, but earth has only 1 moon\"\n",
        "}\n",
        "\n",
        "structured_llm = llm.with_structured_output(schema=Data)\n",
        "structured_llm.invoke([message_no_extraction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp97m49cOUpf",
        "outputId": "46d24c1f-cbed-43fc-98fc-20cf2de349b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Alice', hair_color=None, height_in_meters='1.75')])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm.invoke(messages+[message_no_extraction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRre1zZmOUmn",
        "outputId": "d4b6fb35-abb9-496d-8643-2837b3ecef05"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name=None, hair_color=None, height_in_meters=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}