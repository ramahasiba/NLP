{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqGd4TY3jaDuTPp6VgRZEz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramahasiba/NLP/blob/langGraph/Build_a_Retrieval_Augmented_Generation_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)\n",
        "In this tutorial, we design an aplication that can answer questions about speific source information. These application use a technique known as Retrieval Augmented Generation(RAG).\n",
        "\n",
        "Below we build a simple Q&A application over a text data source."
      ],
      "metadata": {
        "id": "Lyh3xkH14tce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "9ckQR6i6_Qke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZNbYs-G4iAb"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv -q"
      ],
      "metadata": {
        "id": "Bj015EbdBKim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv -q\n",
        "from dotenv import load_dotenv\n",
        "try:\n",
        "  load_dotenv('.env')\n",
        "except ImportError:\n",
        "  print('No .env file found')"
      ],
      "metadata": {
        "id": "RLb_M-7PBIq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangSmith"
      ],
      "metadata": {
        "id": "ijFtrrahAtkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.environ.get(\"LANGSMITH_API_KEY\")"
      ],
      "metadata": {
        "id": "REw04zMc_W1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain[groq]\""
      ],
      "metadata": {
        "id": "8ISLBUI8Bq7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM - Groq"
      ],
      "metadata": {
        "id": "YQQuw0xNMq4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"]=os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "model_name = \"llama3-70b-8192\"\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "llm=init_chat_model(model_name, model_provider=\"groq\")"
      ],
      "metadata": {
        "id": "e6ffTtbJA6Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding from HuggungFace"
      ],
      "metadata": {
        "id": "KSq1Sli0MwFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-huggingface"
      ],
      "metadata": {
        "id": "QGfW4vnHB39X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "-117dqq-Cw2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Database - Chroma DB"
      ],
      "metadata": {
        "id": "IrDQ6cmDM1dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-chroma"
      ],
      "metadata": {
        "id": "MDGSfIUfCwzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"RAG\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_langchain_db\",\n",
        "    # client_settings=settings\n",
        ")"
      ],
      "metadata": {
        "id": "ObpWY05lCwxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This app will answer questions about a website's content. The specific website we used is the [LLM Powered Autonomous Agetns](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post. which allow us to ask questions about the contents of the post."
      ],
      "metadata": {
        "id": "JiX5HN-jhB1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index chunks\n",
        "_ =  vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "# Define prompt for question-answering\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  context: List[Document]\n",
        "  answer: str\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "  retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "  return {\"context\": retrieved_docs}\n",
        "\n",
        "def generate(state: State):\n",
        "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "  messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "  response = llm.invoke(messages)\n",
        "  return {\"answer\": response.content}\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "uMZrtZn4DTpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "id": "5icnLK35FZiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing"
      ],
      "metadata": {
        "id": "4l6v1kgqiT0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Documents"
      ],
      "metadata": {
        "id": "ZDPcdL_6jRN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load() # this returns a list of Document objects\n",
        "\n",
        "assert len(docs) == 1\n",
        "print(f\"Total characters: {len(docs[0].page_content)}\")"
      ],
      "metadata": {
        "id": "vhxDuH4sDTlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "id": "u9E7f004DTjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Documents\n",
        "The loaded document is over 42K, which is too long to fit into the context window of may models. Even if model that could fit the full post in thier context window, models caan struggle to find information in very long inputs."
      ],
      "metadata": {
        "id": "fwLNck7mkLOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RecursiveCharacterTextSplitter is recommended for generic text use cases.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,\n",
        ")\n",
        "\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
      ],
      "metadata": {
        "id": "wxHzoEY0DThX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing Documents\n",
        "\n",
        "Here we index the text chunks so that we can search over them at runtime. we embed the splits and then insert hose embeddings into a vector store."
      ],
      "metadata": {
        "id": "9y8AkfRH-QXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_ids = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "print(document_ids[:4])"
      ],
      "metadata": {
        "id": "_N1_ZvqnDTgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval and Generation\n",
        "Here we used LangGraph to tie together the retrieval and generation steps into a single application."
      ],
      "metadata": {
        "id": "cZJIVYaaCOx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "example_messages = prompt.invoke({\n",
        "    \"context\": \"(context goes here)\",\n",
        "    \"question\": \"(question goes here)\"\n",
        "}).to_messages()\n",
        "\n",
        "assert len(example_messages) == 1\n",
        "print(example_messages[0].content)"
      ],
      "metadata": {
        "id": "RfCDCX4nDTPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the State\n",
        "The state of our application controls what data is input to the application, transferred between steps, and output by the application. For this simple RAG application we keep track of the following:\n",
        "* input question\n",
        "* retrieved context\n",
        "* generated answer"
      ],
      "metadata": {
        "id": "xzjTBdu3MMuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  context: List[Document]\n",
        "  answer: str"
      ],
      "metadata": {
        "id": "SsthjAGsCwvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nodes (application steps)\n",
        "We start with a simple sequence of two steps:\n",
        "* Retrieval\n",
        "* Generation"
      ],
      "metadata": {
        "id": "kzX-cjVjPd4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve( state: State):\n",
        "  # run a similarity search using the input question\n",
        "  retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "  return {\"context\": retrieved_docs}\n",
        "\n",
        "def generate(state: State):\n",
        "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "  # format the retrieved context and original question into a prompt for the chat model\n",
        "  messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "  response = llm.invoke(messages)\n",
        "  return {\"answer\": response.content}"
      ],
      "metadata": {
        "id": "lnZ4zYbd6lWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Control Flow\n",
        "compile the application into a single graph object. Here we just connect the retrieval and generation steps into a single sequence."
      ],
      "metadata": {
        "id": "loCL58GSVZfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "a5-VWBm76lSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "id": "8ycRYd8V6lQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Usage - Testing"
      ],
      "metadata": {
        "id": "ljekeRWOWOpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Invoke:"
      ],
      "metadata": {
        "id": "J5mOI9sQZM0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke\n",
        "result = graph.invoke({\"question\": \"What is task decomposition?\"})\n",
        "\n",
        "print(f\"Context: {result['context']}\")\n",
        "print(f\"Answer: {result['answer']}\")"
      ],
      "metadata": {
        "id": "9m3URsU08_wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke\n",
        "result = graph.invoke({\"question\": \"tell me about task decomposition?\"})\n",
        "\n",
        "print(f\"Context: {result['context']}\")\n",
        "print(f\"Answer: {result['answer']}\")"
      ],
      "metadata": {
        "id": "b2lgZ2jr6nVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming Tokens:"
      ],
      "metadata": {
        "id": "yskBfbGuZJGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in graph.stream(\n",
        "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n",
        "):\n",
        "    print(f\"{step}\\n\\n----------------\\n\")"
      ],
      "metadata": {
        "id": "NXQ4aRKa8_tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message, metadata in graph.stream(\n",
        "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"\n",
        "):\n",
        "    print(message.content, end=\"|\")"
      ],
      "metadata": {
        "id": "9uBioZp-6lMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Customization\n",
        "Customizing the prompt instead of loading it from the prompt hub"
      ],
      "metadata": {
        "id": "LIvwWB8KZcPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "If there is no relevant context, just say that you don't know.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "gGFwNZME-YEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Analysis\n",
        "Here the model rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries."
      ],
      "metadata": {
        "id": "zP_MFLDBZsvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_documents = len(all_splits)\n",
        "third = total_documents // 3\n",
        "\n",
        "for i, document in enumerate(all_splits):\n",
        "    if i < third:\n",
        "        document.metadata[\"section\"] = \"beginning\"\n",
        "    elif i < 2 * third:\n",
        "        document.metadata[\"section\"] = \"middle\"\n",
        "    else:\n",
        "        document.metadata[\"section\"] = \"end\"\n",
        "\n",
        "\n",
        "all_splits[0].metadata"
      ],
      "metadata": {
        "id": "b_udAmy8-YBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "_ = vector_store.add_documents(all_splits)"
      ],
      "metadata": {
        "id": "duMIhTFu-X9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Schema Definition\n",
        "Here we define a schema for the search query. we will use structured putput for this purpose."
      ],
      "metadata": {
        "id": "acnsyrSXjfPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "\n",
        "class Search(TypedDict):\n",
        "    \"\"\"Search query.\"\"\"\n",
        "\n",
        "    query: Annotated[str, ..., \"Search query to run.\"]\n",
        "    section: Annotated[\n",
        "        Literal[\"beginning\", \"middle\", \"end\"],\n",
        "        ...,\n",
        "        \"Section to query.\",\n",
        "    ]"
      ],
      "metadata": {
        "id": "x43ZgRPy-X6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the LangGraph application generate a query from user's raw input."
      ],
      "metadata": {
        "id": "TwgcVoj5vmhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "    question: str\n",
        "    query: Search\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "def analyze_query(state: State):\n",
        "    structured_llm = llm.with_structured_output(Search)\n",
        "    query = structured_llm.invoke(state[\"question\"])\n",
        "    return {\"query\": query}\n",
        "\n",
        "\n",
        "def retrieve(state: State):\n",
        "    query = state[\"query\"]\n",
        "    retrieved_docs = vector_store.similarity_search(\n",
        "        query[\"query\"],\n",
        "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
        "    )\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
        "graph_builder.add_edge(START, \"analyze_query\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "sRVIcmzp-X25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in graph.stream(\n",
        "    {\"question\": \"What does the post say about Task Decomposition?\"},\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    print(f\"{step}\\n\\n----------------\\n\")"
      ],
      "metadata": {
        "id": "1GaSEwxgyR5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note that changing user query may generates error**"
      ],
      "metadata": {
        "id": "81PC4gMRyV9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in graph.stream(\n",
        "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    print(f\"{step}\\n\\n----------------\\n\")"
      ],
      "metadata": {
        "id": "UjmB1IYw6kqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "id": "w5qKx22MvKXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30RQnMbFB45r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}