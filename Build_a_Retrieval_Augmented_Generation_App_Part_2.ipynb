{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramahasiba/NLP/blob/langGraph/Build_a_Retrieval_Augmented_Generation_App_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Build a Retrieval Augmented Generation App Part 2](https://python.langchain.com/docs/tutorials/qa_chat_history/)\n",
        "\n",
        "In this part of building a RAG application, we build an app that allows user to have a back-and-forth conversation, meaning this application has a memory of conversation history.\n",
        "\n",
        "We focus on adding logic for incorporating historical messages and this involves the management of a chat history."
      ],
      "metadata": {
        "id": "GscaMRcw1We6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Installation"
      ],
      "metadata": {
        "id": "UOEzihbmU4Df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE3rPTLQ1Fyv"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \"langchain[groq]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv -q\n",
        "from dotenv import load_dotenv\n",
        "try:\n",
        "  load_dotenv('.env')\n",
        "except ImportError:\n",
        "  print('No .env file found')"
      ],
      "metadata": {
        "id": "z3ASac2x-65R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangSmith"
      ],
      "metadata": {
        "id": "MyjjbO589gRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\n",
        "    \"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.environ.get(\"LANGSMITH_API_KEY\")"
      ],
      "metadata": {
        "id": "-j_arXWQ_Gk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Groq"
      ],
      "metadata": {
        "id": "NhFa9YTx9ivH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"]=os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "model_name = \"llama3-70b-8192\"\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "llm=init_chat_model(model_name, model_provider=\"groq\")"
      ],
      "metadata": {
        "id": "bJlXgD2J_Jab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face"
      ],
      "metadata": {
        "id": "jrTsFGko9kr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-huggingface\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "MKxujeu9_MVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chroma DB"
      ],
      "metadata": {
        "id": "1w69ApKr9nnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-chroma\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"example_collection\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
        ")"
      ],
      "metadata": {
        "id": "NB8PEnzq_Yar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4 -q"
      ],
      "metadata": {
        "id": "1yArGIR7_hkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways we can use to implement ourapplication:\n",
        "* Chains\n",
        "* Agents"
      ],
      "metadata": {
        "id": "TqYYlV5x915z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains\n",
        "Here we execcute at most one retrieval step"
      ],
      "metadata": {
        "id": "pgilkw1VVhIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "# Load chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "all_splits= text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "g67LEqoV_hhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Index chunks\n",
        "_ = vector_store.add_documents(all_splits)"
      ],
      "metadata": {
        "id": "D2Y_BWejAYAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState, StateGraph\n",
        "graph_builder = StateGraph(MessagesState)"
      ],
      "metadata": {
        "id": "e8nIo7y1DCPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve(query: str):\n",
        "  \"\"\"Retrieve information related to a query\"\"\"\n",
        "  retrieved_docs = vector_store.similarity_search(query, k=2)\n",
        "  serialized = \"\\n\\n\".join(\n",
        "      (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\") for doc in retrieved_docs\n",
        "  )\n",
        "  return serialized, retrieved_docs"
      ],
      "metadata": {
        "id": "Mui6JtsQ3nKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph will consist of three noes:\n",
        "* A node that fields the user input, either generating a query for the retriever or responding directly.\n",
        "* A node for retriever tol that executes the retrieval step\n",
        "* A node that generates the final response using the retrieved context"
      ],
      "metadata": {
        "id": "CwIGbLtgjAJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# Step 1: generate AI Message that may include a tool-call to be sent.\n",
        "def query_or_respond(state: MessagesState):\n",
        "  \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
        "  llm_with_tools = llm.bind_tools([retrieve])\n",
        "  response = llm_with_tools.invoke(state[\"messages\"])\n",
        "  # MessagesState appends messages to state instead of overwriting\n",
        "  return {\"messages\": [response]}\n",
        "\n",
        "# Step 2: Execute the retrieval\n",
        "tools = ToolNode([retrieve])\n",
        "\n",
        "# Step 3: Generate a response using the retrieved content.\n",
        "def generate(state: MessagesState):\n",
        "  \"\"\"Generate answer.\"\"\"\n",
        "  # Get generated toolMessagaes\n",
        "  recent_tool_messages = []\n",
        "  for message in reversed(state[\"messages\"]):\n",
        "    if message.type == \"tool\":\n",
        "      recent_tool_messages.append(message)\n",
        "    else:\n",
        "      break\n",
        "  tool_messages = recent_tool_messages[::-1]\n",
        "\n",
        "  # Format into prompt\n",
        "  docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
        "  system_message_content = (\n",
        "      \"You are an assistant for question-answering tasks.\"\n",
        "      \"Use the following pieces of retrieved context to answer\"\n",
        "      \"the question. If you don't know the answer, say that you\"\n",
        "      \"don't know. Use three senences maximum and keep the answer concise.\\n\\n\"\n",
        "      f\"{docs_content}\"\n",
        "  )\n",
        "\n",
        "  conversation_messages =[\n",
        "      message\n",
        "      for message in state[\"messages\"]\n",
        "      if message.type in (\"human\", \"system\")\n",
        "      or (message.type == \"ai\" and not message.tool_calls)\n",
        "  ]\n",
        "  prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
        "\n",
        "  #Run\n",
        "  response = llm.invoke(prompt)\n",
        "  return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "BRj2yhtUDCG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we connect nodes together into a single grap, we allow te first query_or_respond step to \"short-circut\" and respond directly to the user if it does not generate a tool call.\n"
      ],
      "metadata": {
        "id": "ziG-LlkHk-fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "graph_builder.add_node(query_or_respond)\n",
        "graph_builder.add_node(tools)\n",
        "graph_builder.add_node(generate)\n",
        "\n",
        "graph_builder.set_entry_point(\"query_or_respond\")\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"query_or_respond\",\n",
        "    tools_condition,\n",
        "    {END: END, \"tools\": \"tools\"}\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"generate\")\n",
        "graph_builder.add_edge(\"generate\", END)\n",
        "\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "f3F4KA5u_hdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "id": "m0yMHu9TEn3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = SystemMessage(content=\"Only call tools if the user is asking a factual or knowledge-based question. Otherwise, respond directly.\")"
      ],
      "metadata": {
        "id": "DJFTrMftJUf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = \"Hello\"\n",
        "\n",
        "for step in graph.stream(\n",
        "    {\"messages\": [system_message, {\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "JlngYfNNEnzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = \"What is Task Decomposition?\"\n",
        "\n",
        "for step in graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "Tke-BZrBEnw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = \"What is the temprature today?\"\n",
        "\n",
        "for step in graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "3HoJuF7U_haL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "# specify an ID for the thread\n",
        "config = {\"configurable\": {\"thread_id\": \"a\"}}"
      ],
      "metadata": {
        "id": "8rNcVvmoQhXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = \"What is Task Decomposition?\"\n",
        "\n",
        "for step in graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "    config=config,\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "hOLZ5j0MQhUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = \"Can you look up some common ways of doing it?\"\n",
        "\n",
        "for step in graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "    config=config,\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "da_wWi6IQhSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents\n",
        "\n",
        "Agents leverages the reasoning capabilities of LLM to make deccision during execution.\n",
        "\n",
        "Here the tool invokation loops back to the original LLM call. The model can either answerthe question using the retrieved context, or generate another tool call to obtain more information.\n",
        "\n",
        "\n",
        "Here we givw an LLM discretion to execute multiple retrieval steps"
      ],
      "metadata": {
        "id": "ChQbaN8ToDmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)"
      ],
      "metadata": {
        "id": "KW8ihQUp_hW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor"
      ],
      "metadata": {
        "id": "ETmgp2s1_VNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"b\"}}\n",
        "\n",
        "input_message = (\n",
        "    \"what is the standard method for task decomposition?\\n\\n\"\n",
        "    \"Once you get the answer, look up common extensions of that method\"\n",
        ")\n",
        "\n",
        "for event in agent_executor.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "    config=config\n",
        "):\n",
        "  event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "0IgOnpYOS2ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference in those two implementationa is that instead of a final generation step that ends the run that is in the first implementatin, there is a tool invocation in the second implementation that loops back to the original LLM call."
      ],
      "metadata": {
        "id": "mFiHEMRxxwSS"
      }
    }
  ]
}